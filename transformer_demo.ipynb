{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./trans_env/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: requests in ./trans_env/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: torch in ./trans_env/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: tiktoken in ./trans_env/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib in ./trans_env/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: pandas in ./trans_env/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./trans_env/lib/python3.9/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./trans_env/lib/python3.9/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./trans_env/lib/python3.9/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./trans_env/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./trans_env/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./trans_env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./trans_env/lib/python3.9/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: jinja2 in ./trans_env/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx in ./trans_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in ./trans_env/lib/python3.9/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./trans_env/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./trans_env/lib/python3.9/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./trans_env/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./trans_env/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./trans_env/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./trans_env/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in ./trans_env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./trans_env/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Users/roger/Dev/Transformer-from-scratch/trans_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy requests torch tiktoken matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "context_length = 16\n",
    "d_model = 64\n",
    "num_layers = 8\n",
    "num_heads = 4\n",
    "learning_rate = 1e-3\n",
    "dropout = 0.1\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "eval_iters = 20\n",
    "device = ('mps' if torch.backends.mps.is_available() \n",
    "else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "TORCH_SEED = 1337\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: Building Rapport and Capturing Attention\n",
      "Subpoint: Understanding the Importance of Buildi\n",
      "460319\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('sales_textbook.txt'):\n",
    "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
    "    with open('sales_textbook.txt', 'w') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "with open('sales_textbook.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1: Building Rappo\n",
      "[26072, 220, 16, 25, 17283, 23097, 403, 323, 17013, 1711, 63120, 198, 3214, 2837, 25, 46551, 279, 94100, 315, 17283]\n",
      "Tokenized text size: 77919\n",
      "Vocabulary size: 3771\n",
      "The maximum token value in the tokenized text is: 100069\n",
      "min token is: 1\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = encoding.encode(text)\n",
    "vocab_size = len(set(tokenized_text))\n",
    "max_token_value = max(tokenized_text)\n",
    "\n",
    "print(text[:25])\n",
    "print(tokenized_text[:20])\n",
    "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"The maximum token value in the tokenized text is: {max_token_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n",
      "torch.Size([4, 16])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  627,  1383, 88861,   279,  1989,   315, 25607, 16940, 65931,   323,\n",
      "        32097,    11,   584, 26458, 13520,   449], device='mps:0')\n",
      "tensor([ 1383, 88861,   279,  1989,   315, 25607, 16940, 65931,   323, 32097,\n",
      "           11,   584, 26458, 13520,   449,   264], device='mps:0')\n",
      "Token Embedding Lookup Table: torch.Size([100069, 64])\n",
      "torch.Size([4, 16, 64])\n",
      "torch.Size([4, 16, 64])\n",
      "tensor([-7.2645e-01,  2.7959e-01,  6.1379e-01, -6.8169e-01,  6.1964e-01,\n",
      "        -8.3871e-01,  1.6872e+00,  9.2419e-01, -1.7660e+00,  1.0969e+00,\n",
      "         8.2345e-01,  4.0419e-01, -1.0015e+00, -4.0253e-01, -3.6015e-01,\n",
      "        -1.8088e-01,  1.1226e+00, -1.2955e+00, -1.4447e+00,  1.3121e+00,\n",
      "        -6.9775e-01,  1.1749e+00,  5.5158e-01, -1.8591e-01,  1.0946e-01,\n",
      "         7.3421e-01,  1.2775e+00,  1.2458e+00,  2.8220e-01,  1.5083e+00,\n",
      "        -2.1885e-03, -1.7179e+00,  3.7070e-02,  1.2844e-01,  2.3702e-01,\n",
      "         3.5079e-01, -5.3175e-01,  3.4943e-01, -7.5952e-01,  3.6131e-01,\n",
      "         7.6173e-01,  3.4682e-01,  3.4575e-01, -4.1380e-01,  6.2210e-01,\n",
      "        -5.4303e-02, -1.4242e+00,  1.1573e+00, -1.3587e+00,  1.4033e+00,\n",
      "         2.4386e-01, -9.2596e-01,  1.6551e-03, -2.0595e+00,  4.3375e-01,\n",
      "         8.3393e-01, -5.1164e-01,  2.0829e-01,  1.4101e-01,  1.4291e+00,\n",
      "         5.1027e-01,  4.7907e-01,  1.4515e+00, -3.2925e-01], device='mps:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 切分训练和验证集\n",
    "split_idx = int(len(tokenized_text) * 0.8)\n",
    "train_data = tokenized_text[:split_idx] \n",
    "val_data = tokenized_text[split_idx:]\n",
    "\n",
    "data = torch.tensor(train_data, dtype=torch.long, device=device)\n",
    "idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,)) \n",
    "\n",
    "x_batch = torch.stack([data[i:i + context_length] for i in idxs]) \n",
    "y_batch = torch.stack([data[i + 1:i + context_length + 1] for i in idxs])\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)\n",
    "print(x_batch[0])\n",
    "print(y_batch[0])\n",
    "\n",
    "token_embedding_lookup_table = nn.Embedding(max_token_value, d_model).to(device)\n",
    "x = token_embedding_lookup_table(x_batch)\n",
    "y = token_embedding_lookup_table(y_batch)\n",
    "print(\"Token Embedding Lookup Table:\", token_embedding_lookup_table.weight.shape)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Encoding Lookup Table:  torch.Size([4, 16, 64])\n",
      "Final Input Embedding of x:\n",
      "           0         1         2         3         4         5         6   \\\n",
      "0   0.346711  0.416130  0.194183  2.676231  0.104132  0.317640 -0.716679   \n",
      "1   0.115021  0.819889  1.295355  0.050067  1.152809  0.007295  2.096463   \n",
      "2   0.025032 -0.896510  2.007128  0.645062  1.861388  1.183611  0.626594   \n",
      "3   1.540862 -2.196910  1.106263 -0.879057  1.525128 -0.364159  1.305571   \n",
      "4  -1.026346 -2.520314  0.092276 -1.922128  0.772673 -1.248682  0.811819   \n",
      "5  -0.333637 -0.845062 -3.047244 -0.539240  0.370664 -0.112350  0.087253   \n",
      "6  -0.671321  1.424937 -2.025386 -1.031886 -0.385049 -0.656757  0.465220   \n",
      "7   1.460090  1.632251 -2.407473 -0.108412 -1.577922 -1.534449 -0.767310   \n",
      "8   0.493083 -2.413755 -1.205481  0.105365 -0.475036  0.941739  0.546560   \n",
      "9   0.381502 -0.606034  0.249510  1.915294 -0.984113  2.072770 -2.097719   \n",
      "10 -0.586758 -1.718416  1.647630 -0.431370 -0.642387 -0.391723 -0.296238   \n",
      "11 -0.625085 -0.037802  1.642752  1.460065  0.417635  2.481112 -0.515117   \n",
      "12 -0.137083 -1.089729 -0.385102 -1.420898 -0.049978 -0.465174 -1.718115   \n",
      "13  1.090419  0.435371 -0.083390 -1.558577  2.418297  1.322249  0.850477   \n",
      "14  1.396442  0.852242 -2.127405 -2.404308  0.666056 -1.643007 -0.924739   \n",
      "15  0.103718 -0.211813 -1.824957  1.499077  0.716739  0.161400 -0.793259   \n",
      "\n",
      "          7         8         9   ...        54        55        56        57  \\\n",
      "0   2.721329 -1.379066  1.958479  ...  0.326141  0.918822 -0.974995  0.776366   \n",
      "1   1.836588 -1.455025  2.047307  ...  0.434175  1.833931 -0.511319  1.208292   \n",
      "2   0.867692  0.332192  0.616423  ...  0.095568 -0.311354 -1.522920  1.861448   \n",
      "3  -0.088271  0.942626 -0.624826  ... -0.546487  2.515671  1.224692  1.833670   \n",
      "4   0.063149  1.300849 -0.709820  ...  0.412297  0.811908  0.551316  1.974630   \n",
      "5  -1.396884  1.910200  0.589691  ... -2.392044  0.201838 -0.968137  1.399131   \n",
      "6  -2.319048  0.048269 -0.330767  ... -1.924361  1.770508  0.366548  2.237813   \n",
      "7  -3.344894  0.582778 -0.942104  ... -0.624013  2.197189 -0.286863  1.692157   \n",
      "8  -2.336168 -0.348882 -1.902892  ... -1.072307  1.369699 -0.530627  2.117681   \n",
      "9   0.134769 -0.296123 -2.457148  ...  0.307279  0.597253  0.045292  0.127551   \n",
      "10 -1.070488 -0.842712 -1.117289  ...  0.246956  0.162995 -0.534199 -0.385929   \n",
      "11 -1.015839  0.074801 -1.277103  ...  0.305872 -0.519248 -0.783341  1.396661   \n",
      "12 -0.714581  0.168579 -1.108954  ... -0.903096 -1.738268  0.204137  1.928488   \n",
      "13 -0.733657 -1.358562 -1.389894  ... -0.499943  1.458365  0.897332 -0.018865   \n",
      "14  0.318684 -1.985052 -1.211650  ...  0.635039  3.170543 -0.350654  1.756670   \n",
      "15  1.097708  0.100865  2.525651  ...  1.589034  1.928113 -0.097586  2.889562   \n",
      "\n",
      "          58        59        60        61        62        63  \n",
      "0  -1.208962  1.784498  0.213160  2.055345  0.838057  0.250892  \n",
      "1   0.141251  2.429123  0.510447  1.479066  1.451683  0.670746  \n",
      "2  -0.841794  1.096747 -1.112153  1.676283  1.079349  1.205097  \n",
      "3  -0.403483  1.563554  0.591826  0.275255  1.670505  0.758279  \n",
      "4  -0.939264  0.681914 -0.187975  1.821461 -0.166859  0.659965  \n",
      "5  -0.767064  1.607607 -1.890431  2.642382 -0.155402  0.069573  \n",
      "6  -0.125452  0.737559 -2.455601  1.795864  0.514167  0.152089  \n",
      "7  -0.142867  1.440122 -1.049482  1.068458 -0.757912  0.287046  \n",
      "8  -0.386811  0.388143  1.356780  2.472881  0.264217  1.200112  \n",
      "9   1.358625  2.417308 -1.434693  0.254192 -0.241214  0.542596  \n",
      "10 -0.259560  2.623391  0.565274  0.698465  0.255499  1.528143  \n",
      "11 -0.972843  1.512183  1.175235  1.114063  0.163492  1.244288  \n",
      "12  0.637895  0.500249  0.917552  0.887469 -1.344181  0.950669  \n",
      "13  0.955537 -1.200418  0.384898  2.307354 -1.256187 -0.754517  \n",
      "14  0.634567 -0.359409  0.072614  0.528358 -1.816792  1.262165  \n",
      "15 -1.442999  0.118628  0.953136 -1.061797  0.738310  2.549351  \n",
      "\n",
      "[16 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "position_encoding_lookup_table = torch.zeros(context_length, d_model).to(device) # (context_length, d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1) # (context_length, 1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model/2,)  \n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) # (batch_size, context_length, d_model) \n",
    "print(\"Position Encoding Lookup Table: \", position_encoding_lookup_table.shape)\n",
    "\n",
    "input_embedding_x = x + position_encoding_lookup_table # (batch_size, context_length, d_model)\n",
    "input_embedding_y = y + position_encoding_lookup_table # (batch_size, context_length, d_model)\n",
    "X = input_embedding_x # (batch_size, context_length, d_model)\n",
    "x_plot = input_embedding_x[0].detach().cpu().numpy()\n",
    "print(\"Final Input Embedding of x:\\n\", pd.DataFrame(x_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multi-head Attention Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/multihead_attention.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prepare Q,K,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   1.098015  0.205219  1.033230 -0.421223  0.429190 -0.765578 -0.993587   \n",
      "1   0.486800  0.758198  0.342134  1.147735  0.352587  0.913575 -0.240828   \n",
      "2   1.361852  0.288645  0.558850  0.197726 -0.759869 -1.004792 -0.119369   \n",
      "3  -0.319381  1.608541 -0.265403  0.286685 -0.825504 -0.311573 -0.466977   \n",
      "4   0.656908  1.683313 -0.752694 -0.501442 -0.103208 -0.331558 -0.179397   \n",
      "5   0.722305  0.611283 -0.358031 -0.466505 -0.169242  0.115154 -0.692522   \n",
      "6   0.652447  0.972758 -0.640037 -0.757617 -0.713646  0.129324  0.375101   \n",
      "7   0.147175  0.174586  0.415578 -0.213032 -0.499469  1.262464  0.319959   \n",
      "8   0.771177  0.638311 -0.267933  0.029066  0.822012  0.515905  1.583130   \n",
      "9   0.558526  0.459015  0.656071 -0.618360 -0.238946 -0.036866  0.360074   \n",
      "10 -0.719556  1.098709 -0.940380  0.338702 -0.291737 -0.410984 -0.367827   \n",
      "11  1.422824 -0.127827  0.626552 -0.511144  0.215399  0.761427 -0.172003   \n",
      "12  0.232948  1.007835 -0.535164  0.512326  0.564278  0.464594  0.038975   \n",
      "13 -0.317513  0.307166  0.026657  0.090056  0.867442 -0.253929  0.201830   \n",
      "14  0.522285  2.214622 -1.711452  1.420565  2.188150  1.403870  0.104414   \n",
      "15  0.918158  0.332713 -1.028973  1.799485  1.122501  0.864462  1.209446   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.128451  0.352075  1.025810 -0.479727  0.782726  1.179357 -0.957872   \n",
      "1   0.163898  0.235851  0.924242 -0.629698  0.936355  0.107078 -0.579112   \n",
      "2   0.208764  0.786685  0.259074  0.099588  0.444575  0.206759 -0.668491   \n",
      "3  -0.202015  0.887367  0.806262 -1.431273  1.071383 -0.310291 -0.931083   \n",
      "4   0.173719  0.131658  0.136202 -0.088967  0.187402 -0.477934 -1.068629   \n",
      "5  -0.266421 -0.658838  0.473689 -0.750291  0.571639  0.230099 -0.206278   \n",
      "6  -1.059451 -0.022326  0.178938 -0.387163 -0.005254  0.438243 -0.024434   \n",
      "7  -0.329279  0.034975  0.912821 -0.885818 -0.031653 -0.168399 -0.311564   \n",
      "8   0.043406  0.523542  1.257210  0.246528  0.781817  0.746450 -0.380859   \n",
      "9   0.506653  0.277156 -0.447676 -0.568546  0.501493  0.320487 -0.324456   \n",
      "10 -0.288556  0.882411 -0.485498 -0.383891 -0.001069  0.502196  0.275165   \n",
      "11 -0.604792  0.440309  0.056427 -0.042615  0.087952  0.727999 -0.381802   \n",
      "12 -0.253610  0.398028 -0.363330  0.324150 -0.366950 -0.128263  0.254210   \n",
      "13 -0.692542 -0.884760 -0.021462 -1.007295  0.265086 -0.311345  0.439467   \n",
      "14  0.591272  0.359776  0.737796 -0.267305  0.280913  0.354248 -0.664671   \n",
      "15  0.555920  0.504531  2.089715  1.386797  0.877337  1.248601 -0.417719   \n",
      "\n",
      "          14        15  \n",
      "0   0.040186 -0.153710  \n",
      "1   0.031750 -0.794230  \n",
      "2   0.505767 -0.883682  \n",
      "3  -0.187256 -0.994891  \n",
      "4  -0.575026 -0.758938  \n",
      "5  -0.535912 -0.862820  \n",
      "6  -0.370233 -0.112431  \n",
      "7   1.295204 -0.528631  \n",
      "8   0.101129  0.682652  \n",
      "9  -0.532098 -0.059500  \n",
      "10 -0.259247 -0.044727  \n",
      "11 -0.135807 -0.647040  \n",
      "12 -0.078494 -0.518744  \n",
      "13 -0.589808 -0.505073  \n",
      "14 -0.226595 -0.509198  \n",
      "15  0.620321 -0.303728  \n"
     ]
    }
   ],
   "source": [
    "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "\n",
    "Wq = nn.Linear(d_model, d_model).to(device)\n",
    "Wk = nn.Linear(d_model, d_model).to(device)\n",
    "Wv = nn.Linear(d_model, d_model).to(device)\n",
    "\n",
    "Q = Wq(query)\n",
    "Q = Q.view(batch_size, -1, num_heads, d_model // num_heads) # [4, 16, 4, 16] [batch_size, context_length, num_heads, d_model // num_heads]\n",
    "K = Wq(key)\n",
    "K = K.view(batch_size, -1, num_heads, d_model // num_heads) # [4, 16, 4, 16] [batch_size, context_length, num_heads, d_model // num_heads]\n",
    "V = Wv(value)\n",
    "V = V.view(batch_size, -1, num_heads, d_model // num_heads) # [4, 16, 4, 16] [batch_size, context_length, num_heads, d_model // num_heads]\n",
    "\n",
    "# 交换维度位置，将 num_heads 移到第二维，为后续的注意力计算做准备，使得注意力计算可以并行进行\n",
    "Q = Q.transpose(1, 2) # [4, 4, 16, 16] [batch_size, num_heads, context_length, d_model // num_heads]\n",
    "K = K.transpose(1, 2) # [4, 4, 16, 16] [batch_size, num_heads, context_length, d_model // num_heads]\n",
    "V = V.transpose(1, 2) # [4, 4, 16, 16] [batch_size, num_heads, context_length, d_model // num_heads]\n",
    "\n",
    "print(Q.shape)\n",
    "print(pd.DataFrame(Q[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calculate QK^T Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n",
      "          0         1         2          3         4         5         6   \\\n",
      "0   8.619459  3.142893  4.520507   3.242733  1.839295  2.879557  0.559735   \n",
      "1   3.142893  6.451763  2.082761   4.955133  2.036033  2.428303  0.106789   \n",
      "2   4.520507  2.082761  6.354085   3.475078  2.538650  1.368454  0.815163   \n",
      "3   3.242733  4.955133  3.475078  10.501399  5.351847  3.757942  2.637617   \n",
      "4   1.839295  2.036033  2.538650   5.351847  6.621957  3.289231  2.876918   \n",
      "5   2.879557  2.428303  1.368454   3.757942  3.289231  4.508989  2.594900   \n",
      "6   0.559735  0.106789  0.815163   2.637617  2.876918  2.594900  4.669538   \n",
      "7   0.757476  2.951372  0.902415   2.625209 -0.025387  1.103071  1.169249   \n",
      "8   2.191056  2.811849  0.734108   0.660408  1.074552 -0.038380  1.637440   \n",
      "9   2.258942  0.744831  1.753906   1.744321  1.680879  1.185197  1.106795   \n",
      "10 -0.839946  0.034575 -0.145063   3.563659  1.829958  0.376614  1.623793   \n",
      "11  3.621936  2.011119  2.420086   0.204779  0.870457  1.991388  1.778888   \n",
      "12 -1.484427  1.313821 -0.232437   0.795187  1.865901  0.255898  0.854931   \n",
      "13 -0.395792  0.851138 -1.777895   1.246609  0.502940  1.845179  0.696359   \n",
      "14  0.853064  7.122262 -0.862004   4.500677  5.321558  2.495551  1.085567   \n",
      "15  2.231947  6.033042  1.963306   0.433017  0.628035 -0.259016 -0.423599   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.757476  2.191056  2.258942 -0.839946  3.621936 -1.484427 -0.395792   \n",
      "1   2.951372  2.811849  0.744831  0.034575  2.011119  1.313821  0.851138   \n",
      "2   0.902415  0.734108  1.753906 -0.145063  2.420086 -0.232437 -1.777895   \n",
      "3   2.625209  0.660408  1.744321  3.563659  0.204779  0.795187  1.246609   \n",
      "4  -0.025387  1.074552  1.680879  1.829958  0.870457  1.865901  0.502940   \n",
      "5   1.103071 -0.038380  1.185197  0.376614  1.991388  0.255898  1.845179   \n",
      "6   1.169249  1.637440  1.106795  1.623793  1.778888  0.854931  0.696359   \n",
      "7   6.026887  1.526250  0.066097 -1.327504  1.818541 -0.198935 -0.210492   \n",
      "8   1.526250  8.230002  1.009125 -0.669021  1.697362  0.725469 -0.517290   \n",
      "9   0.066097  1.009125  3.126872 -0.025834  1.648313 -0.487907  0.020882   \n",
      "10 -1.327504 -0.669021 -0.025834  4.755391 -1.362468  1.772425  0.304454   \n",
      "11  1.818541  1.697362  1.648313 -1.362468  5.035597  0.491953 -0.454818   \n",
      "12 -0.198935  0.725469 -0.487907  1.772425  0.491953  3.105460  0.514288   \n",
      "13 -0.210492 -0.517290  0.020882  0.304454 -0.454818  0.514288  4.302487   \n",
      "14  1.004852  6.446856 -0.388107  2.861734  0.934370  5.491349  1.747942   \n",
      "15  1.669711  9.421622 -1.625679 -0.743829  1.593989  2.374655 -1.881773   \n",
      "\n",
      "           14         15  \n",
      "0    0.853064   2.231947  \n",
      "1    7.122262   6.033042  \n",
      "2   -0.862004   1.963306  \n",
      "3    4.500677   0.433017  \n",
      "4    5.321558   0.628035  \n",
      "5    2.495551  -0.259016  \n",
      "6    1.085567  -0.423599  \n",
      "7    1.004852   1.669711  \n",
      "8    6.446856   9.421622  \n",
      "9   -0.388107  -1.625679  \n",
      "10   2.861734  -0.743829  \n",
      "11   0.934370   1.593989  \n",
      "12   5.491349   2.374655  \n",
      "13   1.747942  -1.881773  \n",
      "14  18.945818  11.991590  \n",
      "15  11.991590  18.554678  \n"
     ]
    }
   ],
   "source": [
    "attention_score = torch.matmul(Q, K.transpose(-2, -1)) # [4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(attention_score.shape)\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   2.154865  0.785723  1.130127  0.810683  0.459824  0.719889  0.139934   \n",
      "1   0.785723  1.612941  0.520690  1.238783  0.509008  0.607076  0.026697   \n",
      "2   1.130127  0.520690  1.588521  0.868769  0.634663  0.342114  0.203791   \n",
      "3   0.810683  1.238783  0.868769  2.625350  1.337962  0.939486  0.659404   \n",
      "4   0.459824  0.509008  0.634663  1.337962  1.655489  0.822308  0.719229   \n",
      "5   0.719889  0.607076  0.342114  0.939486  0.822308  1.127247  0.648725   \n",
      "6   0.139934  0.026697  0.203791  0.659404  0.719229  0.648725  1.167385   \n",
      "7   0.189369  0.737843  0.225604  0.656302 -0.006347  0.275768  0.292312   \n",
      "8   0.547764  0.702962  0.183527  0.165102  0.268638 -0.009595  0.409360   \n",
      "9   0.564735  0.186208  0.438477  0.436080  0.420220  0.296299  0.276699   \n",
      "10 -0.209987  0.008644 -0.036266  0.890915  0.457490  0.094153  0.405948   \n",
      "11  0.905484  0.502780  0.605021  0.051195  0.217614  0.497847  0.444722   \n",
      "12 -0.371107  0.328455 -0.058109  0.198797  0.466475  0.063975  0.213733   \n",
      "13 -0.098948  0.212785 -0.444474  0.311652  0.125735  0.461295  0.174090   \n",
      "14  0.213266  1.780566 -0.215501  1.125169  1.330389  0.623888  0.271392   \n",
      "15  0.557987  1.508260  0.490826  0.108254  0.157009 -0.064754 -0.105900   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.189369  0.547764  0.564735 -0.209987  0.905484 -0.371107 -0.098948   \n",
      "1   0.737843  0.702962  0.186208  0.008644  0.502780  0.328455  0.212785   \n",
      "2   0.225604  0.183527  0.438477 -0.036266  0.605021 -0.058109 -0.444474   \n",
      "3   0.656302  0.165102  0.436080  0.890915  0.051195  0.198797  0.311652   \n",
      "4  -0.006347  0.268638  0.420220  0.457490  0.217614  0.466475  0.125735   \n",
      "5   0.275768 -0.009595  0.296299  0.094153  0.497847  0.063975  0.461295   \n",
      "6   0.292312  0.409360  0.276699  0.405948  0.444722  0.213733  0.174090   \n",
      "7   1.506722  0.381562  0.016524 -0.331876  0.454635 -0.049734 -0.052623   \n",
      "8   0.381562  2.057501  0.252281 -0.167255  0.424341  0.181367 -0.129323   \n",
      "9   0.016524  0.252281  0.781718 -0.006458  0.412078 -0.121977  0.005221   \n",
      "10 -0.331876 -0.167255 -0.006458  1.188848 -0.340617  0.443106  0.076114   \n",
      "11  0.454635  0.424341  0.412078 -0.340617  1.258899  0.122988 -0.113704   \n",
      "12 -0.049734  0.181367 -0.121977  0.443106  0.122988  0.776365  0.128572   \n",
      "13 -0.052623 -0.129323  0.005221  0.076114 -0.113704  0.128572  1.075622   \n",
      "14  0.251213  1.611714 -0.097027  0.715433  0.233593  1.372837  0.436986   \n",
      "15  0.417428  2.355406 -0.406420 -0.185957  0.398497  0.593664 -0.470443   \n",
      "\n",
      "          14        15  \n",
      "0   0.213266  0.557987  \n",
      "1   1.780566  1.508260  \n",
      "2  -0.215501  0.490826  \n",
      "3   1.125169  0.108254  \n",
      "4   1.330389  0.157009  \n",
      "5   0.623888 -0.064754  \n",
      "6   0.271392 -0.105900  \n",
      "7   0.251213  0.417428  \n",
      "8   1.611714  2.355406  \n",
      "9  -0.097027 -0.406420  \n",
      "10  0.715433 -0.185957  \n",
      "11  0.233593  0.398497  \n",
      "12  1.372837  0.593664  \n",
      "13  0.436986 -0.470443  \n",
      "14  4.736454  2.997897  \n",
      "15  2.997897  4.638669  \n"
     ]
    }
   ],
   "source": [
    "attention_score = attention_score / math.sqrt(d_model // num_heads)\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   2.154865      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1   0.785723  1.612941      -inf      -inf      -inf      -inf      -inf   \n",
      "2   1.130127  0.520690  1.588521      -inf      -inf      -inf      -inf   \n",
      "3   0.810683  1.238783  0.868769  2.625350      -inf      -inf      -inf   \n",
      "4   0.459824  0.509008  0.634663  1.337962  1.655489      -inf      -inf   \n",
      "5   0.719889  0.607076  0.342114  0.939486  0.822308  1.127247      -inf   \n",
      "6   0.139934  0.026697  0.203791  0.659404  0.719229  0.648725  1.167385   \n",
      "7   0.189369  0.737843  0.225604  0.656302 -0.006347  0.275768  0.292312   \n",
      "8   0.547764  0.702962  0.183527  0.165102  0.268638 -0.009595  0.409360   \n",
      "9   0.564735  0.186208  0.438477  0.436080  0.420220  0.296299  0.276699   \n",
      "10 -0.209987  0.008644 -0.036266  0.890915  0.457490  0.094153  0.405948   \n",
      "11  0.905484  0.502780  0.605021  0.051195  0.217614  0.497847  0.444722   \n",
      "12 -0.371107  0.328455 -0.058109  0.198797  0.466475  0.063975  0.213733   \n",
      "13 -0.098948  0.212785 -0.444474  0.311652  0.125735  0.461295  0.174090   \n",
      "14  0.213266  1.780566 -0.215501  1.125169  1.330389  0.623888  0.271392   \n",
      "15  0.557987  1.508260  0.490826  0.108254  0.157009 -0.064754 -0.105900   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "1       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "2       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "3       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "4       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "5       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "6       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "7   1.506722      -inf      -inf      -inf      -inf      -inf      -inf   \n",
      "8   0.381562  2.057501      -inf      -inf      -inf      -inf      -inf   \n",
      "9   0.016524  0.252281  0.781718      -inf      -inf      -inf      -inf   \n",
      "10 -0.331876 -0.167255 -0.006458  1.188848      -inf      -inf      -inf   \n",
      "11  0.454635  0.424341  0.412078 -0.340617  1.258899      -inf      -inf   \n",
      "12 -0.049734  0.181367 -0.121977  0.443106  0.122988  0.776365      -inf   \n",
      "13 -0.052623 -0.129323  0.005221  0.076114 -0.113704  0.128572  1.075622   \n",
      "14  0.251213  1.611714 -0.097027  0.715433  0.233593  1.372837  0.436986   \n",
      "15  0.417428  2.355406 -0.406420 -0.185957  0.398497  0.593664 -0.470443   \n",
      "\n",
      "          14        15  \n",
      "0       -inf      -inf  \n",
      "1       -inf      -inf  \n",
      "2       -inf      -inf  \n",
      "3       -inf      -inf  \n",
      "4       -inf      -inf  \n",
      "5       -inf      -inf  \n",
      "6       -inf      -inf  \n",
      "7       -inf      -inf  \n",
      "8       -inf      -inf  \n",
      "9       -inf      -inf  \n",
      "10      -inf      -inf  \n",
      "11      -inf      -inf  \n",
      "12      -inf      -inf  \n",
      "13      -inf      -inf  \n",
      "14  4.736454      -inf  \n",
      "15  2.997897  4.638669  \n"
     ]
    }
   ],
   "source": [
    "attention_score = attention_score.masked_fill(\n",
    "        torch.triu(torch.ones(attention_score.shape[-2:]).to(device), diagonal=1).bool(), \n",
    "        float('-inf')) # 将矩阵的上三角设置为-inf, 形状为[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6   \\\n",
      "0   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.304234  0.695766  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.319981  0.173960  0.506060  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.102741  0.157640  0.108886  0.630732  0.000000  0.000000  0.000000   \n",
      "4   0.111686  0.117317  0.133025  0.268764  0.369208  0.000000  0.000000   \n",
      "5   0.155401  0.138822  0.106509  0.193563  0.172161  0.233543  0.000000   \n",
      "6   0.091895  0.082056  0.097954  0.154488  0.164012  0.152847  0.256748   \n",
      "7   0.082528  0.142824  0.085574  0.131641  0.067859  0.089976  0.091477   \n",
      "8   0.089952  0.105055  0.062492  0.061351  0.068044  0.051518  0.078326   \n",
      "9   0.119371  0.081754  0.105212  0.104960  0.103309  0.091268  0.089497   \n",
      "10  0.053185  0.066182  0.063276  0.159921  0.103675  0.072090  0.098466   \n",
      "11  0.121795  0.081421  0.090186  0.051834  0.061220  0.081021  0.076829   \n",
      "12  0.043020  0.086593  0.058830  0.076063  0.099409  0.066469  0.077208   \n",
      "13  0.053492  0.073058  0.037864  0.080650  0.066968  0.093669  0.070285   \n",
      "14  0.008360  0.040074  0.005445  0.020808  0.025548  0.012605  0.008860   \n",
      "15  0.011423  0.029544  0.010681  0.007285  0.007649  0.006128  0.005881   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.308122  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.076179  0.407083  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.068994  0.087338  0.148297  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.047082  0.055507  0.065190  0.215425  0.000000  0.000000  0.000000   \n",
      "11  0.077594  0.075279  0.074361  0.035031  0.173427  0.000000  0.000000   \n",
      "12  0.059325  0.074749  0.055190  0.097113  0.070510  0.135522  0.000000   \n",
      "13  0.056028  0.051891  0.059364  0.063726  0.052708  0.067158  0.173139   \n",
      "14  0.008683  0.033848  0.006130  0.013813  0.008532  0.026656  0.010456   \n",
      "15  0.009925  0.068926  0.004354  0.005429  0.009739  0.011838  0.004084   \n",
      "\n",
      "          14       15  \n",
      "0   0.000000  0.00000  \n",
      "1   0.000000  0.00000  \n",
      "2   0.000000  0.00000  \n",
      "3   0.000000  0.00000  \n",
      "4   0.000000  0.00000  \n",
      "5   0.000000  0.00000  \n",
      "6   0.000000  0.00000  \n",
      "7   0.000000  0.00000  \n",
      "8   0.000000  0.00000  \n",
      "9   0.000000  0.00000  \n",
      "10  0.000000  0.00000  \n",
      "11  0.000000  0.00000  \n",
      "12  0.000000  0.00000  \n",
      "13  0.000000  0.00000  \n",
      "14  0.770183  0.00000  \n",
      "15  0.131043  0.67607  \n"
     ]
    }
   ],
   "source": [
    "attention_score = torch.softmax(attention_score, dim=-1) # [4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
    "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Calculate V Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 16])\n",
      "torch.Size([4, 4, 16, 16])\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   1.404491  0.273069 -1.009697  1.125089  0.448860 -0.699946 -0.194428   \n",
      "1   1.039182  0.530752 -0.623729  1.028149  0.048530 -0.715421 -0.089486   \n",
      "2   0.333371  0.114961 -0.981869  1.175668 -0.042304 -0.108881  0.517010   \n",
      "3   0.421122  0.940021 -0.920434  1.004752 -0.253904 -1.109372  0.136975   \n",
      "4   0.133389  0.445351 -0.638007  0.601900 -0.233886 -0.635158  0.202332   \n",
      "5   0.164900  0.477558 -0.511829  0.587863 -0.161267 -0.498483  0.268524   \n",
      "6  -0.026703  0.591076 -0.166389  0.355832  0.050325 -0.386237  0.341763   \n",
      "7  -0.109981  0.730334 -0.580128  0.493912 -0.014560 -0.267685  0.515086   \n",
      "8   0.145014  0.400024 -0.521913  0.314520  0.208060 -0.132144 -0.363337   \n",
      "9   0.135645  0.550333 -0.413918  0.371470  0.161968 -0.244217  0.123829   \n",
      "10  0.149946  0.645187 -0.412956  0.225751 -0.083613 -0.395455 -0.138985   \n",
      "11 -0.029733  0.386741 -0.423626  0.476684  0.193222 -0.093940  0.147260   \n",
      "12 -0.073404  0.303509 -0.385656  0.185507  0.018278 -0.157082 -0.030132   \n",
      "13 -0.142686  0.388254 -0.367662  0.285098  0.375681 -0.177730  0.185832   \n",
      "14 -0.455170  0.544328 -1.426628  0.920933  1.014315 -0.941483 -0.904294   \n",
      "15 -0.264736 -0.581434 -1.162704  0.885462  0.551282 -0.572066 -1.127211   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0  -0.061803  0.117205 -1.427982 -0.666706 -0.925039 -0.033322  0.251366   \n",
      "1  -0.034432  0.088162 -0.920790 -0.589213 -0.258567  0.354227 -0.888575   \n",
      "2   0.320011 -0.095142 -0.698346 -0.625183 -0.336503 -0.611498 -0.240272   \n",
      "3   0.367184 -0.171010 -0.342078 -0.498799 -0.516294 -0.915814  0.310540   \n",
      "4   0.127619 -0.470815 -0.191670 -0.522764 -0.414816 -0.675627 -0.142787   \n",
      "5   0.275888 -0.357106 -0.491379 -0.463098 -0.592071 -0.379032 -0.095465   \n",
      "6   0.249484 -0.289974 -0.537628 -0.170540 -0.437314 -0.210540  0.137684   \n",
      "7   0.365727  0.062889 -0.479092 -0.122157 -0.259159 -0.306444  0.143765   \n",
      "8   0.686407 -0.193556 -0.331052 -0.099080 -0.555387  0.066701  0.153084   \n",
      "9   0.433732 -0.167676 -0.488538 -0.280138 -0.435929 -0.185189  0.175816   \n",
      "10  0.394143 -0.392103 -0.422237 -0.571786 -0.401232 -0.250585  0.241057   \n",
      "11  0.556045 -0.067964 -0.429976 -0.230633 -0.358469 -0.162839  0.242714   \n",
      "12  0.352644 -0.328253 -0.329956 -0.273778 -0.345710 -0.205232  0.087046   \n",
      "13  0.637585 -0.174312 -0.252410 -0.169653 -0.106355 -0.093092  0.136126   \n",
      "14  0.565227 -0.376089 -0.074163  0.202614  0.926784  0.244761 -0.811338   \n",
      "15 -0.857476 -0.524803 -0.625454  0.632996 -0.208658  0.242988 -0.798583   \n",
      "\n",
      "          14        15  \n",
      "0   0.813949 -0.355896  \n",
      "1   0.454560 -0.576301  \n",
      "2   0.587608 -0.908477  \n",
      "3   0.892174 -1.284178  \n",
      "4   0.668700 -0.952096  \n",
      "5   0.592752 -0.863902  \n",
      "6   0.554595 -0.976159  \n",
      "7   0.366607 -0.758593  \n",
      "8   0.318937 -0.983330  \n",
      "9   0.531604 -0.837408  \n",
      "10  0.485265 -0.939698  \n",
      "11  0.585808 -0.688326  \n",
      "12  0.520532 -0.874534  \n",
      "13  0.472784 -0.397782  \n",
      "14  0.433552  0.210441  \n",
      "15  1.111774 -0.717237  \n"
     ]
    }
   ],
   "source": [
    "A = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, d_model // num_heads]\n",
    "print(attention_score.shape)\n",
    "print(A.shape)\n",
    "print(pd.DataFrame(A[0][0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Concatenate and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n",
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "A = A.transpose(1, 2) # [4, 4, 16, 16] -> [4, 16, 4, 16] [batch_size, context_length, num_heads, d_model // num_heads]\n",
    "A = A.reshape(batch_size, -1, d_model) # [4, 16, 4, 16] -> [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(A.shape)\n",
    "\n",
    "Wo = nn.Linear(d_model, d_model).to(device) # [64, 64] [d_model, d_model]\n",
    "output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Residual Connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# Add residual connection\n",
    "output = output + X\n",
    "\n",
    "# Add Layer Normalization\n",
    "layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "output = layer_norm(output) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n",
      "torch.Size([4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "# 保存输入，用于后续残差连接\n",
    "layer_norm_output = output\n",
    "# 升维\n",
    "output = nn.Linear(d_model, d_model * 4).to(device)(output) # [4, 16, 256] [batch_size, context_length, d_model * 4]\n",
    "# 激活函数\n",
    "output = nn.ReLU().to(device)(output)\n",
    "# 降维\n",
    "output = nn.Linear(d_model * 4, d_model).to(device)(output) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "# dropout防止过拟合\n",
    "output = torch.dropout(output, p=dropout, train=True) # [4, 16, 64] [batch_size, context_length, d_model]\n",
    "print(output.shape)\n",
    "\n",
    "# 残差连接\n",
    "output = output + layer_norm_output\n",
    "# 层归一化\n",
    "layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "output = layer_norm(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Repeat step 4 to 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step8: Output Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 100070])\n",
      "      0         1         2         3         4         5         6       \\\n",
      "0   0.000003  0.000006  0.000010  0.000015  0.000018  0.000010  0.000007   \n",
      "1   0.000005  0.000007  0.000010  0.000008  0.000010  0.000005  0.000006   \n",
      "2   0.000013  0.000019  0.000005  0.000009  0.000007  0.000014  0.000005   \n",
      "3   0.000011  0.000017  0.000012  0.000004  0.000005  0.000012  0.000008   \n",
      "4   0.000013  0.000015  0.000018  0.000004  0.000010  0.000008  0.000011   \n",
      "5   0.000008  0.000015  0.000012  0.000004  0.000010  0.000012  0.000004   \n",
      "6   0.000012  0.000014  0.000010  0.000005  0.000003  0.000008  0.000003   \n",
      "7   0.000020  0.000009  0.000016  0.000005  0.000007  0.000004  0.000008   \n",
      "8   0.000020  0.000015  0.000009  0.000005  0.000006  0.000003  0.000009   \n",
      "9   0.000008  0.000008  0.000010  0.000016  0.000009  0.000002  0.000004   \n",
      "10  0.000006  0.000011  0.000010  0.000003  0.000007  0.000007  0.000008   \n",
      "11  0.000006  0.000026  0.000008  0.000015  0.000016  0.000011  0.000014   \n",
      "12  0.000013  0.000016  0.000022  0.000009  0.000006  0.000003  0.000017   \n",
      "13  0.000012  0.000016  0.000010  0.000021  0.000005  0.000011  0.000008   \n",
      "14  0.000014  0.000007  0.000020  0.000016  0.000005  0.000004  0.000007   \n",
      "15  0.000005  0.000013  0.000031  0.000013  0.000007  0.000010  0.000009   \n",
      "\n",
      "      7         8         9       ...    100060    100061    100062    100063  \\\n",
      "0   0.000009  0.000007  0.000008  ...  0.000021  0.000023  0.000016  0.000012   \n",
      "1   0.000013  0.000014  0.000009  ...  0.000010  0.000045  0.000008  0.000015   \n",
      "2   0.000007  0.000004  0.000012  ...  0.000009  0.000008  0.000004  0.000015   \n",
      "3   0.000016  0.000006  0.000010  ...  0.000017  0.000031  0.000006  0.000016   \n",
      "4   0.000004  0.000002  0.000008  ...  0.000021  0.000025  0.000005  0.000016   \n",
      "5   0.000005  0.000002  0.000010  ...  0.000027  0.000007  0.000005  0.000009   \n",
      "6   0.000012  0.000008  0.000007  ...  0.000048  0.000010  0.000008  0.000004   \n",
      "7   0.000019  0.000004  0.000004  ...  0.000020  0.000011  0.000003  0.000013   \n",
      "8   0.000015  0.000006  0.000008  ...  0.000016  0.000013  0.000002  0.000010   \n",
      "9   0.000009  0.000008  0.000003  ...  0.000016  0.000008  0.000005  0.000011   \n",
      "10  0.000005  0.000016  0.000009  ...  0.000011  0.000031  0.000004  0.000011   \n",
      "11  0.000019  0.000005  0.000012  ...  0.000004  0.000017  0.000007  0.000024   \n",
      "12  0.000010  0.000003  0.000007  ...  0.000017  0.000022  0.000003  0.000013   \n",
      "13  0.000015  0.000007  0.000008  ...  0.000021  0.000021  0.000004  0.000004   \n",
      "14  0.000005  0.000004  0.000007  ...  0.000015  0.000019  0.000006  0.000012   \n",
      "15  0.000008  0.000006  0.000011  ...  0.000008  0.000016  0.000007  0.000008   \n",
      "\n",
      "      100064    100065    100066    100067    100068    100069  \n",
      "0   0.000005  0.000028  0.000014  0.000005  0.000005  0.000003  \n",
      "1   0.000021  0.000015  0.000010  0.000004  0.000012  0.000004  \n",
      "2   0.000008  0.000004  0.000008  0.000003  0.000008  0.000002  \n",
      "3   0.000026  0.000011  0.000018  0.000003  0.000015  0.000005  \n",
      "4   0.000015  0.000006  0.000017  0.000005  0.000018  0.000005  \n",
      "5   0.000010  0.000010  0.000016  0.000010  0.000008  0.000005  \n",
      "6   0.000012  0.000008  0.000018  0.000008  0.000030  0.000014  \n",
      "7   0.000022  0.000019  0.000014  0.000010  0.000007  0.000010  \n",
      "8   0.000013  0.000015  0.000007  0.000007  0.000007  0.000007  \n",
      "9   0.000005  0.000013  0.000012  0.000007  0.000012  0.000020  \n",
      "10  0.000011  0.000017  0.000007  0.000004  0.000005  0.000007  \n",
      "11  0.000009  0.000009  0.000017  0.000009  0.000017  0.000003  \n",
      "12  0.000006  0.000008  0.000009  0.000005  0.000014  0.000009  \n",
      "13  0.000012  0.000014  0.000007  0.000006  0.000016  0.000005  \n",
      "14  0.000005  0.000010  0.000005  0.000008  0.000011  0.000004  \n",
      "15  0.000005  0.000013  0.000010  0.000006  0.000007  0.000005  \n",
      "\n",
      "[16 rows x 100070 columns]\n",
      "predict token:  tensor([11173, 73966, 67801, 27206, 77680, 87637, 15608, 91886, 49700, 10208,\n",
      "        52410, 91220, 34606, 53943, 83113, 41225], device='mps:0')\n",
      "predict text:  _flag_iters Ideally Horn vér/on=\"#\">olahFuse.Q Eine statusBar PandBindings\tpc.FC\n"
     ]
    }
   ],
   "source": [
    "# 通过线性层，映射到词表，词表从0开始编码，长度为 max_token_value + 1\n",
    "logist = nn.Linear(d_model, max_token_value + 1).to(device)(output) # [4, 16, 100070] [batch_size, context_length, max_token_value + 1]\n",
    "print(logist.shape)\n",
    "# 计算每一个词的概率分布\n",
    "probabilities = torch.softmax(logist, dim=-1) \n",
    "print(pd.DataFrame(probabilities[0].detach().cpu().numpy()))\n",
    "\n",
    "predicted_token = torch.argmax(probabilities[0], dim=-1)\n",
    "print(\"predict token: \", predicted_token)\n",
    "predicted_text = encoding.decode(predicted_token.cpu().tolist())\n",
    "print(\"predict text: \", predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
